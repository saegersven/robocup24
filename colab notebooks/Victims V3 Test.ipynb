{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGtAd1e7wELO3/kifbZ6On"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Initialization"],"metadata":{"id":"EwU2x0m1XXnH"}},{"cell_type":"code","source":["from google.colab import drive\n","import numpy as np\n","import os\n","import cv2\n","import tensorflow as tf\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import BatchNormalization, Conv2D, Softmax, Reshape\n","from tensorflow.keras.models import Model\n","import shutil\n","import zipfile\n","\n","drive.mount('/content/drive')\n","\n","num_classes = 2\n","ARCHIVE_PATH = '/content/drive/MyDrive/data24/victims_corner/archives'\n","CSV_PATH = '/content/drive/MyDrive/data24/victims_corner/csv'\n","\n","LOCAL_CSV_PATH = '/content/data/csv'\n","INPUT_IMAGE_PATH = '/content/data/input_images'\n","MODEL_INPUT_IMAGE_PATH = '/content/data/model_input_images'\n","TARGET_OUTPUT_PATH = '/content/data/targets'\n","\n","MODEL_OUTPUT_PATH = '/content/drive/MyDrive/victims_v3.tflite'\n","\n","os.makedirs(LOCAL_CSV_PATH, exist_ok=True)\n","os.makedirs(INPUT_IMAGE_PATH, exist_ok=True)\n","os.makedirs(MODEL_INPUT_IMAGE_PATH, exist_ok=True)\n","os.makedirs(TARGET_OUTPUT_PATH, exist_ok=True)\n","\n","def remove_quotation_marks(x: str):\n","  return x.replace('\"', '').strip()\n","\n","input_height = 96\n","input_width = 96\n","input_shape = (input_height, input_width, 1)\n","\n","target_width = 12\n","target_height = 12"],"metadata":{"id":"bAiO-m9RUID9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716718864198,"user_tz":-120,"elapsed":1793,"user":{"displayName":"Team BitFlip","userId":"04411406670819963962"}},"outputId":"e4f90871-a92e-47cf-8978-be26bed87e1e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#Load dataset"],"metadata":{"id":"ysdlWpuUXD4W"}},{"cell_type":"markdown","source":["##Unpack archives, copy CSVs"],"metadata":{"id":"zUTFBKgBZdhD"}},{"cell_type":"code","source":["for archive in os.listdir(ARCHIVE_PATH):\n","  print(f'Unpacking {archive}...')\n","  with zipfile.ZipFile(os.path.join(ARCHIVE_PATH, archive)) as zip_file:\n","    for member in zip_file.namelist():\n","      filename = os.path.basename(member)\n","\n","      if not filename:\n","        continue\n","\n","      source = zip_file.open(member)\n","      target = open(os.path.join(INPUT_IMAGE_PATH, filename), 'wb')\n","      with source, target:\n","        shutil.copyfileobj(source, target)\n","\n","for csv in os.listdir(CSV_PATH):\n","  print(f'Copying {csv}...')\n","  with open(os.path.join(CSV_PATH, csv), 'r') as f:\n","    f2 = open(os.path.join(LOCAL_CSV_PATH, csv), 'w')\n","    f2.write(f.read())\n","    f2.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eg4NKAYYLev","executionInfo":{"status":"ok","timestamp":1716718491404,"user_tz":-120,"elapsed":19955,"user":{"displayName":"Team BitFlip","userId":"04411406670819963962"}},"outputId":"64692dea-3954-46bd-fff7-309b25dd96e3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Unpacking victims_corner_2023_12_28.zip...\n","Unpacking victims_2023_12_27.zip...\n","Unpacking 20240223.zip...\n","Unpacking 20240224.zip...\n","Unpacking victims_2024_02_28.zip...\n","Copying victims_2023_12_27.csv...\n","Copying victims_2023_12_28.csv...\n","Copying victims_2024_02_23-export.csv...\n","Copying victims_28_02_2024-export.csv...\n"]}]},{"cell_type":"markdown","source":["##Resize input for model"],"metadata":{"id":"7_6Hf4ACZffD"}},{"cell_type":"code","source":["for filename in os.listdir(INPUT_IMAGE_PATH):\n","  img = cv2.imread(os.path.join(INPUT_IMAGE_PATH, filename))\n","  if img is None or img.shape != (240, 320, 3):\n","    print(f'{filename} is wrong size.')\n","    continue\n","  img = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (input_width, input_height))\n","  cv2.imwrite(os.path.join(MODEL_INPUT_IMAGE_PATH, filename), img)"],"metadata":{"id":"TJoCpxavaxuz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Generate targets"],"metadata":{"id":"AUkMMfL-a052"}},{"cell_type":"code","source":["targets = {}\n","empty_target = np.full((target_width, target_height, 3), [128, -127, -127], dtype=np.int8)"],"metadata":{"id":"JRvYUW0jZcWf","executionInfo":{"status":"ok","timestamp":1716718963981,"user_tz":-120,"elapsed":211,"user":{"displayName":"Team BitFlip","userId":"04411406670819963962"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["#Build model"],"metadata":{"id":"rYXkBwawYKKa"}},{"cell_type":"code","source":["num_classes_with_background = num_classes + 1\n","\n","width, height, input_channels = input_shape\n","\n","if width != height:\n","  raise Exception(f\"Only square inputs are supperted; not {input_shape}\")\n","\n","mobile_net_v2 = MobileNetV2(input_shape=input_shape,\n","                            alpha=alpha,\n","                            include_top=True)\n","for layer in mobile_net_v2.layers:\n","  if type(layer) == BatchNormalization:\n","    layer.momentum = 0.9\n","\n","cut_point = mobile_net_v2.get_layer('block_5_expand_relu')\n","\n","model = Conv2D(filters=32, kernel_size=1, strides=1,\n","                activation='relu', name='head')(cut_point.output)\n","logits = Conv2D(filters=num_classes_with_background, kernel_size=1, strides=1,\n","                activation=None, name='logits')(model)\n","\n","model = Model(inputs=mobile_net_v2.input, outputs=logits)\n","\n","model_output_shape = model.layers[-1].output.shape\n","_batch, width, height, num_classes = model_output_shape\n","if width != height:\n","  raise Exception(f\"Only square outputs are supperted; not {model_output_shape}\")\n","output_width_height = width\n","\n","prefetch_policy = tf.data.experimental.AUTOTUNE\n","\n","train_segmentation_dataset = as_segmentation(train_dataset, True)\n","validation_segmentation_dataset = as_segmentation(validation_dataset, False)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(learning_rate=learning_rate))\n","\n","model.fit(train_segmentation_dataset, validation_data=validation_segmentation_dataset,\n","          epochs=num_epochs, verbose=0)\n","\n","softmax_layer = Softmax()(model.layers[-1].output)\n","model = Model(model.input, softmax_layer)"],"metadata":{"id":"r4EOro8TXDfb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train"],"metadata":{"id":"-Np4s1obXFvo"}},{"cell_type":"code","source":["model = train(num_classes=2,\n","              learning_rate=0.001,\n","              num_epochs=20,\n","              alpha=0.35,\n","              object_weight=100,\n","              train_dataset=train_dataset,\n","              validation_dataset=validation_dataset,\n","              best_model_path='',\n","              input_shape=input_shape,\n","              batch_size=32)"],"metadata":{"id":"cWRCK5uCWmq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Test"],"metadata":{"id":"86L6u3s8XG6K"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12DpFgg1pKgKF5fqvf_hFJZD3xmTIUrxT"},"id":"hWPpR7qXfqOV","executionInfo":{"status":"ok","timestamp":1716657902214,"user_tz":-120,"elapsed":10245,"user":{"displayName":"Team BitFlip","userId":"04411406670819963962"}},"outputId":"73a19a59-b03d-44d8-d745-2822edd7ccb1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["from google.colab import drive\n","\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","\n","drive.mount('/content/drive')\n","\n","imgpath = '/content/drive/MyDrive/data24/victims_corner/manual_test'\n","modelpath = '/content/drive/MyDrive/allgood2.lite'\n","\n","num_test_images = 8\n","\n","# Grab filenames of all images in test folder\n","images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","# Load the label map into memory\n","labels = ['living', 'dead']\n","\n","# Load the Tensorflow Lite model into memory\n","interpreter = Interpreter(model_path=modelpath)\n","interpreter.allocate_tensors()\n","\n","# Get model details\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","height = input_details[0]['shape'][1]\n","width = input_details[0]['shape'][2]\n","\n","print(input_details)\n","\n","print(f\"Model expects {width}x{height} image\")\n","\n","float_input = (input_details[0]['dtype'] == np.float32)\n","\n","input_mean = 127.5\n","input_std = 127.5\n","\n","# Randomly select test images\n","images_to_test = random.sample(images, num_test_images)\n","\n","# Loop over every image and perform detection\n","for image_path in images_to_test:\n","\n","    # Load image and resize to expected shape [1xHxWx3]\n","    image = cv2.imread(image_path)\n","    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    imH, imW, _ = image.shape\n","    image_resized = cv2.resize(image_gray, (width, height))\n","    input_data = np.expand_dims(np.expand_dims(image_resized, axis=0), axis=3)\n","    print(input_data.shape)\n","\n","    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","    if float_input:\n","        input_data = (np.float32(input_data) - input_mean) / input_std\n","    else:\n","      # Convert uint8 to int8\n","      input_data = np.int8(input_data - input_mean)\n","\n","    # Perform the actual detection by running the model with the image as input\n","    interpreter.set_tensor(input_details[0]['index'],input_data)\n","\n","    start_time = time.time()\n","    interpreter.invoke()\n","\n","    end_time = time.time()\n","\n","    # Retrieve detection results\n","    boxes = interpreter.get_tensor(output_details[0]['index'])[0,:,:,:] # Bounding box coordinates of detected objects\n","    classes = interpreter.get_tensor(output_details[0]['index'])[0] # Class index of detected objects\n","    scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","    output_width = boxes.shape[1]\n","    output_height = boxes.shape[0]\n","    input_width = image.shape[1]\n","    input_height = image.shape[0]\n","\n","    boxes = boxes + 128\n","\n","    print(\"Time: \", end_time - start_time)\n","\n","    detections = []\n","\n","    contours_living, _ = cv2.findContours(cv2.threshold(np.uint8(boxes[:,:,2]), 180, 255, cv2.THRESH_BINARY)[1], cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_dead, _ = cv2.findContours(cv2.threshold(np.uint8(boxes[:,:,1]), 180, 255, cv2.THRESH_BINARY)[1], cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    for arr, label in [(contours_living, True), (contours_dead, False)]:\n","      for i in range(len(arr)):\n","        x, y, w, h = cv2.boundingRect(arr[i])\n","        detections.append((x * input_width / output_width, y * input_height / output_height, label))\n","\n","    for detection in detections:\n","      color = (255, 0, 0)\n","      if detection[2]:\n","        color = (0, 255, 0)\n","\n","      cv2.circle(image, (int(detection[0]), int(detection[1])), 5, color, 3)\n","\n","      print(detection)\n","\n","\n","    plt.figure(figsize=(12, 16))\n","    plt.imshow(image)\n","\n","    # Loop over all detections and draw detection box if confidence is above minimum threshold\n","    \"\"\"for i in range(len(scores)):\n","        if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","\n","            # Get bounding box coordinates and draw box\n","            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","            ymin = int(max(1,(boxes[i][0] * imH)))\n","            xmin = int(max(1,(boxes[i][1] * imW)))\n","            ymax = int(min(imH,(boxes[i][2] * imH)))\n","            xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","            cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 1)\n","\n","            # Draw label\n","            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n","            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1) # Get font size\n","            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","            cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","            cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1) # Draw label text\n","\n","            detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\"\"\"\n"]}]}